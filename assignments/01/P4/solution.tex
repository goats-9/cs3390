\documentclass[journal,12pt,twocolumn]{IEEEtran}
\usepackage{setspace}
\usepackage{gensymb}
\singlespacing
\usepackage[cmex10]{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} % loads  TikZ and tkz-base
\usepackage{listings}
\usepackage{color}    
\usepackage{array}    
\usepackage{longtable}
\usepackage{calc}     
\usepackage{multirow} 
\usepackage{hhline}   
\usepackage{ifthen}   
\usepackage{lscape}     
\usepackage{chngcntr}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\DeclareMathOperator*{\Res}{Res}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}
\renewcommand\thetable{\arabic{table}}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}                                 %%

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible,
literate=
{-}{$\rightarrow{}$}{1},
}
%\lstset{
%language=tex,
%frame=single, 
%breaklines=true
%}
\setlength{\parindent}{0pt}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}   
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
\providecommand{\system}[1]{\overset{\mathcal{#1}}{ \longleftrightarrow}}
\providecommand{\grad}[2]{\ensuremath{\mathbf{\nabla}_{#1}#2}}
\providecommand{\pdiff}[2]{\ensuremath{\frac{\partial #2}{\partial #1}}}
\providecommand{\diff}[2]{\ensuremath{\frac{d #2}{d #1}}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\newcommand{\cosec}{\,\text{cosec}\,}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}

\vspace{3cm}
\title{CS3390 Assignment 1\\Problem 4}
\author{Gautam Singh (CS21BTECH11018)\\Jaswanth Beere (BM21BTECH11007)}
\maketitle
\tableofcontents
\bigskip

This document discusses an algorithm to find a solution to a binary
classification problem that uses the sigmoid function in logistic regression.

\section{Newton-Raphson Update Equation}

Define

\begin{equation}
     p_i \triangleq \pr{y_i = 1|\vec{x}_i, \vec{w}} = \frac{1}{1 + \exp\brak{-\vec{w}^\top\vec{x}_i}}
     \label{eq:pi-def}
\end{equation}

The likelihood function for logistic regression is given by

\begin{equation}
     L\brak{\vec{w}} = \prod_{i:y_i = 1}p_i\prod_{j:y_j = 0}\brak{1-p_j}
     \label{eq:Lw}
\end{equation}

Hence, the log likelihood is given by

\begin{equation}
     l\brak{\vec{w}} = \sum_{i=1}^{N}y_i\log p_i + \brak{1 - y_i}\log\brak{1 - p_i}
     \label{eq:lw}
\end{equation}

and using \eqref{eq:pi-def}, the error function is simply the negative of
\eqref{eq:lw}, given by

\begin{align}
     E\brak{\vec{w}} &= -\sum_{i=1}^{N}y_i\log p_i + \brak{1 - y_i}\log\brak{1 - p_i} \\
                     &= -\sum_{i=1}^{N}y_i\brak{\vec{w}^\top\vec{x}_i} - \log\brak{1 + \exp\brak{\vec{w}^\top\vec{x}_i}}
                     \label{eq:err-def}
\end{align}

Using \eqref{eq:pi-def}, the first derivatives of \eqref{eq:err-def} are

\begin{align}
     \pdiff{w_r}{E} &= -\sum_{i=1}^{N}\brak{y_i - \frac{1}{1 + \exp\brak{-\vec{w}^\top\vec{x}_i}}}x_{ir} \\
                    &= -\sum_{i=1}^{N}\brak{y_i - p_i}x_{ir}.
                    \label{eq:pdiff-err}
\end{align}

The second derivatives are therefore

\begin{align}
     \frac{\partial^2E}{\partial w_r \partial w_s} &= \sum_{i=1}^{N}x_{ir}x_{is}\frac{\exp\brak{-\vec{w}^\top\vec{x}}}{\brak{1 + \exp\brak{-\vec{w}^\top\vec{x}_i}}^2} \\
                                                   &= \sum_{i=1}^{N}x_{ir}x_{is}p_i\brak{1-p_i}.
     \label{eq:pdiff2-err}
\end{align}

Hence, from \eqref{eq:pdiff-err} the gradient of \eqref{eq:err-def} is

\begin{equation}
     \nabla E\brak{\vec{w}} = -\sum_{i=1}^{N}\brak{y_i - p_i}\vec{x}_i = -\vec{X}^\top\brak{\vec{y}-\vec{p}}     
     \label{eq:grad-err}
\end{equation}

and from \eqref{eq:pdiff2-err}, the Hessian becomes

\begin{equation}
     \vec{H}_E\brak{\vec{w}} = \vec{X}^\top\vec{WX},
     \label{eq:hess-err}
\end{equation}

where we define

\begin{align}
     \vec{X} &= \myvec{\vec{x}_1 & \vec{x}_2 & \ldots & \vec{x}_N} \label{eq:X-def} \\
     \vec{y} &= \myvec{y_1 & y_2 & \ldots & y_N}^\top \label{eq:y-def} \\
     \vec{p} &= \myvec{p_1 & p_2 & \ldots & p_N}^\top \label{eq:p-def} \\
     \vec{W} &= \mathrm{diag}\myvec{p_1\brak{1-p_1} & \ldots & p_N\brak{1-p_N}}. \label{eq:W-def}
\end{align}

Thus, the Hessian update equation becomes

\begin{align}
     \vec{w}_{n+1} &= \vec{w}_n - \vec{H}_E\brak{\vec{w}_n}^{-1}\nabla E\brak{\vec{w}_n} \\
                   &= \vec{w}_n + \brak{\vec{X}^\top\vec{WX}}^{-1}\vec{X}^\top\brak{\vec{y}-\vec{p}}.
                   \label{eq:hess-upd}
\end{align}

The algorithm for performing such a computation is depicted below.

\begin{algorithm}
     \caption{Newton-Raphson Update Algorithm for Logistic Regression}\label{alg:nr-log-reg}
     \begin{algorithmic}[1]
          \Function{NR-Logistic-Regression}{$\vec{X}$, $\vec{y}$}
          \State \(\brak{r,c} \gets \mathrm{dim}\brak{\vec{X}}\)
          \State let \(\vec{w}\) be an initial guess for \(\vec{\hat{w}}_{ML}\)
          \Repeat
               \State compute \(\vec{p}\) and \(\vec{W}\) as in \eqref{eq:p-def} and \eqref{eq:W-def}
               \State \(\vec{w} \gets \vec{w} + \brak{\vec{X}^\top\vec{WX}}^{-1}\vec{X}^\top\brak{\vec{y}-\vec{p}}\)
          \Until{(the change in \(\vec{w}\) is small enough)}
          \EndFunction
          \Return \(\vec{w}\)
     \end{algorithmic}
\end{algorithm}

\section{Relation to Weighted Least Squares}

The optimal solution \(\vec{\hat{w}}_{ML}\), given by

\begin{equation}
     \vec{\hat{w}}_{ML} = \brak{\vec{X}^\top\vec{WX}}^{-1}\vec{X}^\top\brak{\vec{y}-\vec{p}}.
     \label{eq:w-ml-log-reg}
\end{equation}

Note that the optimal solution is quite similar to the one for reweighted least
squares regression in Problem 3, where \(\vec{y}-\vec{p}\) is the ``corrected''
outputs with the probabilities of success being the correction. However, the
diagonal matrix \(\vec{W}\) is a function of \(\vec{w}\), hence this algorithm
is known as the \emph{iterative reweighted least squares} (IRLS) method.

\section{Convexity of Error Function}

For all \(i\), \(p_i\brak{1-p_i} \ge 0\), thus the diagonal entries of
\(\vec{W}\) are all positive. Hence, from \eqref{eq:hess-err}, it follows that
the eigenvalues of \(\vec{H}_E\), which are the diagonal entries in \(\vec{W}\)
are nonnegative for any parameters \(\vec{w}\). Thus, \(\vec{H}_E\) is positive
semidefinite and therefore, \(E\brak{\vec{w}}\) is convex. Hence, it has a
unique minimum at the point defined in \eqref{eq:w-ml-log-reg}.

\end{document}
