\documentclass[journal,12pt,twocolumn]{IEEEtran}
\usepackage{setspace}
\usepackage{gensymb}
\singlespacing
\usepackage[cmex10]{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{bm}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{circuitikz}
\usepackage{verbatim}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} % loads  TikZ and tkz-base
\usepackage{listings}
\usepackage{color}    
\usepackage{array}    
\usepackage{longtable}
\usepackage{calc}     
\usepackage{multirow} 
\usepackage{hhline}   
\usepackage{ifthen}   
\usepackage{lscape}     
\usepackage{chngcntr}
\DeclareMathOperator*{\Res}{Res}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}
\renewcommand\thetable{\arabic{table}}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\def\inputGnumericTable{}                                 %%

\lstset{
%language=C,
frame=single, 
breaklines=true,
columns=fullflexible,
literate=
{-}{$\rightarrow{}$}{1},
}
%\lstset{
%language=tex,
%frame=single, 
%breaklines=true
%}
\setlength{\parindent}{0pt}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\begin{document}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\bibliographystyle{IEEEtran}
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}   
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
\providecommand{\system}[1]{\overset{\mathcal{#1}}{ \longleftrightarrow}}
\providecommand{\grad}[2]{\ensuremath{\mathbf{\nabla}_{#1}#2}}
\providecommand{\pdiff}[2]{\ensuremath{\frac{\partial #2}{\partial #1}}}
\providecommand{\diff}[2]{\ensuremath{\frac{d #2}{d #1}}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\newcommand{\cosec}{\,\text{cosec}\,}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\renewcommand{\vec}[1]{\boldsymbol{\mathbf{#1}}}
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}

\vspace{3cm}
\title{CS3390 Assignment 1\\Problem 2}
\author{Gautam Singh\\CS21BTECH11018}
\maketitle
\tableofcontents
\bigskip

This document summarises and demonstrates the methods described in \cite{PMC80}
using the dataset in \cite{misc_wine_quality_186}.

\section{Summary}

Data is said to be \emph{ordinal} if it can be grouped into ordered categories.
These categories can be thought of as intervals of a function of some
underlying, unknown continuous random variable, where these intervals can be of
any distance and non-continuous. Two models for performing regression on ordinal
data, namely the \emph{proportional odds} and \emph{proportional hazards} model,
are described in \cite{PMC80} along with their practical use cases. Suppose that
the response variable belongs to \(k\) ordered categories with probabilities
\(\pi_j\brak{\vec{x}}\). In both cases, the models are of the form

\begin{equation}
     \mathrm{link}\cbrak{\gamma_j\brak{\vec{x}}} = \theta_j - \vec{\beta}^\top\vec{x}
     \label{eq:ord-reg-model}
\end{equation}

which is a general linear model where 

\begin{enumerate}
     \item \(\gamma_j\brak{\vec{x}} = \sum_{i=1}^j \pi_i\brak{\vec{x}}\) denotes
     the probability that the response variable falls in category at most \(j\)
     for inputs \(\vec{x}\).
     \item \(\vec{\beta}\) and \(\theta_j,\ 1 \le j < k\) are the parameters of the model to
     be estimated. Usually, \(\theta_j\) are referred to as \emph{cut points}.
     \item \(\mathrm{link}\) is a monotone function mapping \(\brak{0,1}\) to
     \(\brak{-\infty,\infty}\). The link function should be selected based on
     ease of interpretation for the application.
\end{enumerate}

Also studied in this paper are nonlinear models of the form

\begin{equation}
     \mathrm{link}\cbrak{\gamma_j\brak{\vec{x_i}}} = \frac{\theta_j - \vec{\beta}^\top\vec{x_i}}{\tau_i}
     \label{eq:nl-ord-reg-model}
\end{equation}

which are useful for applications where the covariates \(\vec{x}\) may not have
the same ``variance''. Here, \(\tau_i\) is called the \emph{scale} of the
\(i\)th row of a dataset, and is given by

\begin{equation}
     \log\tau_i = \vec{\tau}^\top\brak{\vec{x_i}-\bar{\vec{x}}}
\end{equation}

where \(\vec{\tau}\) is a vector of parameters of the model.

In both cases, it turns out that numerical methods of solving these models, in
particular reweighted least squares, converges to the maximum likelihood
estimate. The paper also contrasts the performance of ordinal regression with
application-specific qualitative tests and alternative models for various
healthcare-related applications, illustrating that models such as
\eqref{eq:ord-reg-model} are easy to compute and interpret.

\section{Differences From Other Methods}

It is tempting to consider the ordinal categories as classes and use a
classification model. However, in cases where the categories are models of
underlying continuous random variables and processes, it is better to use
ordinal regression. Using classification methods is amenable only when the
proportion of classes in the sample dataset and population dataset are equal,
which is not always possible. Classification in such cases can lead to
indicators with poor generalization performance.

On the other hand, regression is suitable for applications where the response
variable is real-valued. Obviously, applying regression directly for ordinal
data will not work, since the data is discrete and grouped into categories.
Further, it is difficult to obtain information about the underlying model or
process which puts the response variable into these intervals. Therefore,
ordinal regression makes use of a link function which makes the model amenable
to the same tools as for linear regression, such as least squares and gradient
descent.

\section{Maximum Likelihood Parameter Estimation}

Suppose that the categorical responses in a dataset of \(n\) inputs are
distributed as \(\cbrak{n_1, n_2, \ldots, n_k}\). Define for \(1 \le j \le k\),

\begin{equation}
     R_j \triangleq \sum_{i=1}^j n_i,\ Z_j \triangleq \frac{R_j}{n}.
     \label{eq:rz-j-def}
\end{equation}

Using the definitions of \(\gamma_j\) described earlier, define

\begin{align}
     \phi_j &\triangleq \log\brak{\frac{\gamma_j}{\gamma_{j+1}-\gamma_j}} = \mathrm{logit}\brak{\frac{\gamma_j}{\gamma_{j+1}}} \label{eq:phi-def} \\
     g\brak{\phi_j} &\triangleq \log\brak{1 + \exp\brak{\phi_j}} = \log\brak{\frac{\gamma_{j+1}}{\gamma_{j+1}-\gamma_j}}.
     \label{eq:gphi-def}
\end{align}

From \eqref{eq:phi-def}, we have

\begin{align}
     \pdiff{\gamma_j}{\phi_j} &= \brak{\frac{\gamma_{j+1}-\gamma_j}{\gamma_j}}\frac{\brak{\gamma_{j+1}-\gamma_j} - \brak{-\gamma_j}}{\brak{\gamma_{j+1}-\gamma_j}^2} \\
                              &= \frac{\gamma_{j+1}}{\gamma_j\brak{\gamma_{j+1}-\gamma_j}} \\
     \pdiff{\gamma_{j+1}}{\phi_j} &= \brak{\frac{\gamma_{j+1}-\gamma_j}{\gamma_j}}\frac{-\gamma_j}{\brak{\gamma_{j+1}-\gamma_j}^2} \\
                              &= \frac{-1}{\gamma_{j+1}-\gamma_j} \\
     \implies \pdiff{\gamma_j}{\phi_j} &= -\frac{\gamma_{j+1}}{\gamma_j}\pdiff{\gamma_j}{\phi_j}
     \label{eq:pdiff-phi-rel}
\end{align}

Noting that \(\gamma_k = 1\), the likelihood can be written as

\begin{align}
     L &= \prod_{j=1}^{k}\pi_j^{n_i} \\
       &= \gamma_1^{R_1}\prod_{j=1}^{k-1}\brak{\gamma_{j+1}-\gamma_i}^{R_{j+1}-R_j} \\
       &= \gamma_1^{R_1}\prod_{j=1}^{k-1}\gamma_{j+1}^{R_{j+1}}\brak{\frac{1}{\gamma_{j+1}^{R_j}}}\brak{\frac{\gamma_{j+1}-\gamma_j}{\gamma_{j+1}}}^{R_{j+1}-R_j} \\
       &= \prod_{j=1}^{k-1}\brak{\frac{\gamma_j}{\gamma_{j+1}}}^{R_j}\brak{\frac{\gamma_{j+1}-\gamma_j}{\gamma_{j+1}}}^{R_{j+1}-R_j} \\
       &= \prod_{j=1}^{k-1}\brak{\frac{\gamma_j}{\gamma_{j+1}-\gamma_j}}^{R_j}\brak{\frac{\gamma_{j+1}-\gamma_j}{\gamma_{j+1}}}^{R_{j+1}}.
       \label{eq:ord-reg-L}
\end{align}

Therefore, using \eqref{eq:rz-j-def}, the log-likelihood is

\begin{equation}
     l = n\sum_{i=1}^{k-1}\brak{Z_i\phi_i - Z_{i+1}g\brak{\phi_i}}.
     \label{eq:ord-reg-l}
\end{equation}

To find the maximum likelihood estimate of \(\theta_j,\ \vec{\beta}\) and
\(\vec{\tau}\), \cite{PMC80} recommends the use of the Newton-Raphson Method
with Fisher scoring. This method converges rapidly even for poor initial
estimates, provided that the \(\theta_j\) are monotone increasing.

Define

\begin{equation}
     \vec{\beta}^* \triangleq \myvec{\theta_1 & \theta_2 & \ldots & \theta_{k-1} & \beta_1 & \ldots & \beta_p}^\top \\
     \label{eq:beta-star-def}
\end{equation}

and denote the entire parameter vector as

\begin{equation}
     \vec{\psi} \triangleq \myvec{\vec{\beta}^* \\ \vec{\tau}}.
     \label{eq:psi-def}
\end{equation}

Define

\begin{align}
     \vec{X}^*_j &\triangleq \myvec{\vec{e}_j \\ \vec{x}} \\
     \vec{U} &\triangleq \vec{\bar{x}} - \vec{x} \\
     w &\triangleq \exp\brak{\vec{\tau}^\top\vec{U}}.
\end{align}

Then, \eqref{eq:nl-ord-reg-model} can be rewritten as

\begin{equation}
     Y_j = \mathrm{link}\brak{\gamma_j} = \vec{\beta}^{*\top}\vec{X}^*_j\exp\brak{\vec{\tau}^\top\vec{U}}.
     \label{eq:nl-ord-reg-ll}
\end{equation}

Here, \(\vec{e}_j^\top\) denotes the \(j\)th standard basic vector in
\(\mathbb{R}^{k-1}\). 

From \eqref{eq:nl-ord-reg-ll}, we have

\begin{align}
     \pdiff{\beta^*_r}{Y_j} &= X^*_{jr}\exp\brak{\vec{\tau}^\top\vec{U}} = wX^*_{jr} \label{eq:pdiff-beta-Y} \\
     \pdiff{\tau_r}{Y_j} &= U_r\brak{\vec{\beta}^{*\top}\vec{X}^*_j\exp\brak{\vec{\tau}^\top\vec{U}}} = Y_jU_r \label{eq:pdiff-tau-Y}
\end{align}

Using the chain rule,

\begin{align}
     \pdiff{\beta^*_r}{l} &= \sum_{j=1}^{k-1}\pdiff{\phi_j}{l}\pdiff{\beta^*_r}{\phi_j} \\
                          &= \sum_{j=1}^{k-1}\pdiff{\phi_j}{l}\brak{\pdiff{\gamma_j}{\phi_j}\pdiff{\beta^*_r}{\gamma_j} + \pdiff{\gamma_{j+1}}{\phi_j}\pdiff{\beta^*_r}{\gamma_{j+1}}} \\ 
                          &= \sum_{j=1}^{k-1}\pdiff{\phi_j}{l}\brak{\pdiff{\gamma_j}{\phi_j}\pdiff{Y_j}{\gamma_j}\pdiff{\beta_r^*}{Y_j} + \pdiff{\gamma_{j+1}}{\phi_j}\pdiff{Y_{j+1}}{\gamma_{j+1}}\pdiff{\beta^*_r}{Y_{j+1}}} \\ 
                          &= w\sum_{j=1}^{k-1}\pdiff{\phi_j}{l}\pdiff{\gamma_j}{\phi_j}\brak{\pdiff{Y_j}{\gamma_j}X_{jr}^* - \frac{\gamma_j}{\gamma_{j+1}}\pdiff{Y_{j+1}}{\gamma_{j+1}}X_{j+1,r}^*} \\
                          &= w\sum_{j=1}^{k-1}\pdiff{\phi_j}{l}V_j^{-1}q_{jr}
                          \label{eq:pdiff-beta-l}
\end{align}

where

\begin{align}
     V_j &\triangleq \pdiff{\phi_j}{\gamma_j} \label{eq:V-def} \\
     q_{jr} &\triangleq \pdiff{Y_j}{\gamma_j}X_{jr}^* - \frac{\gamma_j}{\gamma_{j+1}}\pdiff{Y_{j+1}}{\gamma_{j+1}}X_{j+1,r}^*. \label{eq:q-def}
\end{align}

From \eqref{eq:phi-def}, \eqref{eq:gphi-def} and \eqref{eq:ord-reg-l}, we have,
\begin{align}
     \pdiff{\phi_j}{l} &= n\brak{Z_j - Z_{j+1}\pdiff{\phi_j}{g\brak{\phi_j}}} \\
                       &= n\brak{Z_j - Z_{j+1}\frac{\gamma_j}{\gamma_{j+1}}}.
                       \label{eq:pdiff-phi-l}
\end{align}

Thus, using \eqref{eq:pdiff-beta-Y} and \eqref{eq:q-def},

\begin{align}
     \frac{\partial^2l}{\partial \beta_r^* \partial \phi_j} &= -nwZ_{j+1}\frac{\gamma_{j+1}\pdiff{Y_j}{\gamma_j}X_{jr}^* - \gamma_j\pdiff{Y_{j+1}}{\gamma_{j+1}}X_{j+1,r}^*}{\gamma_{j+1}^2} \\
                                                            &= -nw\frac{Z_{j+1}}{\gamma_{j+1}}q_{jr}.
                                                            \label{eq:pdiff-beta-phi-l}
\end{align}

Notice that \(V_j\) and \(q_{jr}\) are independent of \(\beta^*_s,\ s \neq r\).
Differentiating \(l\) twice, from \eqref{eq:pdiff-beta-l} and
\eqref{eq:pdiff-beta-phi-l},

\begin{align}
     \frac{\partial^2l}{\partial \beta_r^* \partial \beta_s^*} &= -nw^2\sum_{j=1}^{k-1}V_j^{-1}\frac{Z_{j+1}}{\gamma_{j+1}}q_{jr}q_{js} \\
     \implies \mean{\frac{\partial^2l}{\partial \beta_r^* \partial \beta_s^*}} &= -nw^2\sum_{j=1}^{k-1}V_j^{-1}q_{jr}q_{js}
     \label{eq:pdiff2-beta-l}
\end{align}

since by \eqref{eq:rz-j-def}, \(\mean{Z_{j+1}} = \gamma_{j+1}\).

Using \eqref{eq:pdiff-tau-Y}, \eqref{eq:pdiff-phi-rel} and the chain rule in a
similar manner, we obtain

\begin{align}
     \pdiff{\tau_r}{l} &= U_r\sum_{j=1}^{k-1}\pdiff{\phi_j}{l}\pdiff{\gamma_j}{\phi_j}\brak{\pdiff{Y_j}{\gamma_j}Y_j - \frac{\gamma_j}{\gamma_{j+1}}\pdiff{Y_{j+1}}{\gamma_{j+1}}Y_{j+1}} \\
                       &= U_r\sum_{j=1}^{k-1}\pdiff{\phi_j}{l}V_j^{-1}q_j
                       \label{eq:pdiff-tau-l}
\end{align}

where we define

\begin{equation}
     q_j \triangleq \pdiff{Y_j}{\gamma_j}Y_j - \frac{\gamma_j}{\gamma_{j+1}}\pdiff{Y_{j+1}}{\gamma_{j+1}}Y_{j+1}.
     \label{eq:qj-def}
\end{equation}

Thus, the expected second derivative turns out to be

\begin{equation}
     \mean{\frac{\partial^2l}{\partial \tau_r \partial \tau_s}} = -nU_rU_s\sum_{j=1}^{k-1}V_j^{-1}q_j^2.
     \label{eq:pdiff2-tau-l}
\end{equation}

Similarly, the mixed expected second derivatives are

\begin{equation}
     \mean{\frac{\partial^2l}{\partial \beta_r^* \partial \tau_s}} = -nwU_s\sum_{j=1}^{k-1}V_j^{-1}q_jq_{jr}.
     \label{eq:pdiff2-beta-tau}
\end{equation}

Thus, the negative expectation of the Hessian matrix \(\vec{A}_l =
-\mean{\vec{H}_l}\) is symmetric and from \eqref{eq:pdiff2-beta-l} and
\eqref{eq:pdiff2-tau-l}, has nonnegative diagonal entries. Therefore, it is
positive semidefinite.

Applying a Taylor series expansion for \(\nabla_{\vec{\psi}}l\) around
\(\vec{\psi}_n\) using the expected Hessian matrix, we get

\begin{equation}
     \nabla l\brak{\vec{\psi}} = \nabla l\brak{\vec{\psi}_n} - \vec{A}_l\brak{\vec{\psi} - \vec{\psi}_n} + \ldots
     \label{eq:grad-l-taylor}
\end{equation}

Using a linear approximation, and taking \(\nabla l\brak{\vec{\psi}} = \vec{0}\)
at \(\vec{\psi} = \vec{\psi}_{n+1}\), we get the update equation

\begin{align}
     \vec{A}_l\brak{\vec{\psi}_{n+1}-\vec{\psi}_n} &= \nabla l\brak{\vec{\psi}_n} \\
     \implies \vec{\psi}_{n+1} &= \vec{\psi}_n + \vec{A}_l^{-1}\nabla l\brak{\vec{\psi}_n}.
     \label{eq:nr-fisher-update}
\end{align}

where the entries of \(\vec{A}_l\) are defined in \eqref{eq:pdiff2-beta-l},
\eqref{eq:pdiff2-tau-l} and \eqref{eq:pdiff2-beta-tau}.

\bibliography{references}

\end{document}
