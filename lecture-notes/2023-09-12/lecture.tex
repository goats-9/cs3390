% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%

\documentclass[twoside]{article}
%\usepackage{soul}
\usepackage{./lecnotes_macros}


\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{CS3390}{Foundations of Machine Learning}{7}{12 September 2023}{P. K.
Srijith}{Gautam Singh}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

%All figures are to be placed in a separate folder named ``images''

% **** YOUR NOTES GO HERE:

\section{Probabilistic Generative Models}

So far, we have considered the following classification methods.

\begin{enumerate} 
    \item Using posterior class probabilities, called \emph{discriminative
        models}.
    \item Find a function \(f\), called a discriminant function, which maps each
        input \(\vec{x}\) directly onto a class label. Probabilities play no
        role here. 
\end{enumerate}

\subsection{Generative Classifiers}

Here, we first solve the inference problem of determining the class-conditional
densities \(p\brak{\vec{x}|y}\). Also, we require the prior class probabilities
\(p\brak{y}\). Then, we use Bayes' Theorem as follows.

\begin{equation} 
    p\brak{y|\vec{x}} = \frac{p\brak{\vec{x}|y}p\brak{y}}{p\brak{\vec{x}}}
\end{equation}

Approaches that implicitly or explicitly model the distribution of inputs and
outputs are called \textbf{generative models}, because by sampling from them we
can generate synthetic data points in the input space.

Approaches that model the posterior probabilities directly are called
\textbf{discriminative models}.

\subsubsection{Binary Classification}
For two classes, we have

\begin{align} 
    p\brak{\cC_1|\vec{x}} &= \frac{p\brak{\vec{x}|\cC_1}p\brak{\cC_1}}{p\brak{\vec{x}|\cC_1}p\brak{\cC_1} + p\brak{\vec{x}|\cC_2}p\brak{\cC_2}} \\
                          &= \frac{1}{1 + \exp\brak{-a}}
    \label{eq:gen-sigmoid} 
\end{align}

where

\begin{equation}
    a \triangleq \ln\frac{p\brak{\vec{x}|\cC_1}p\brak{\cC_1}}{p\brak{\vec{x}|\cC_2}p\brak{\cC_2}}.
    \label{eq:a-sigmoid-def}
\end{equation}

\subsubsection{Multiclass Classification}
If there are \(k\) classes,

\begin{align} 
    p\brak{\cC_k|\vec{x}} &= \frac{p\brak{\vec{x}|\cC_k}p\brak{\cC_k}}{\sum_i
    p\brak{\vec{x}|\cC_k}p\brak{\cC_k}} \\ &=
    \frac{\exp\brak{a_k}}{\sum_j\exp{a_j}} 
    \label{eq:gen-softmax}
\end{align}

where

\begin{equation}
    a_k \triangleq \ln p\brak{\vec{x}|\cC-k}p\brak{\cC_k}
    \label{eq:ak-softmax-def}
\end{equation}

Hence,

\begin{equation}
    p\brak{\vec{x}|\cC_k} = \frac{1}{\brak{2\pi}^{\frac{D}{2}}\abs{\vec{\Sigma}}^{\frac{1}{2}}}\exp\cbrak{-\frac{1}{2}\brak{\vec{x}-\vec{\mu_k}^\top\vec{\Sigma}^{-1}\brak{\vec{x}-\vec{\mu_k}}}}
    \label{eq:bayes-norm}
\end{equation}

and also,

\begin{equation}
    \vec{w_k} = \vec{\Sigma}^{-1}\vec{\mu_k}
    \label{eq:w-k-def}
\end{equation}

\section{Discriminant Analysis: Parameter Estimation}

We model joint probability of observing input and output.

\begin{align}
    p\brak{\vec{x_n},\cC_1} &= p\brak{\cC_1}p\brak{\vec{x_n}|\cC_1} = \pi\cN\brak{\vec{x_n}|\vec{\mu_1},\vec{Sigma}} \\
    p\brak{\vec{x_n},\cC_2} &= p\brak{\cC_2}p\brak{\vec{x_n}|\cC_2} = \brak{1-\pi}\cN\brak{\vec{x_n}|\vec{\mu_2},\vec{Sigma}}
\end{align}

Then, the likelihood is given by

\begin{equation}
    p\brak{\vec{t}|\pi,\vec{\mu_1},\vec{\mu_2},\vec{\Sigma}} = \prod_{i=1}^N\sbrak{\pi\cN\brak{\vec{x_n}|\vec{\mu_1},\vec{\Sigma}}}^{t_n}\sbrak{\brak{1-\pi}\cN\brak{\vec{x_n}|\vec{\mu_2},\vec{\Sigma}}}^{1-t_n}
    \label{eq:parameter-likelihood}
\end{equation}

Taking the logarithm on both sides of \eqref{eq:parameter-likelihood}

\end{document}
